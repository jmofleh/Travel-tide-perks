# -*- coding: utf-8 -*-
"""user_pca_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOiCTuZaS6do-PHnOjAS6vFgY6CRp7uP

#PCA Analysis
Principal Component Analysis (PCA) is a method for dimensionality reduction. Its goal is to reduce the complexity of datasets with many variables (dimensions) by summarizing the most important information into a smaller number of new, uncorrelated variables, known as principal components. This helps to identify and visualize patterns in the data.
"""

# import necessary libraries
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Connect to google drive-Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# read the data file from related directory
directory = "/content/drive/MyDrive/Travel tide project"

df_users = pd.read_csv(f'{directory}/user_base.csv', index_col="user_id")
print(df_users.columns)

"""# To prepare our data for clustering, I encoded demographic categorical and boolean columns into numeric values (1 and 0) so they can be used in machine learning models."""

# To prepare our data for clustering, I encoded demographic categorical and boolean columns into numeric values (1 and 0) so
# they can be used in machine learning models.
df_users["gender"] = df_users["gender"].map({"F": 0, "M": 1, "O":2})
df_users["married"] = df_users["married"].astype(int)
df_users["has_children"] = df_users["has_children"].astype(int)

print('different countries', df_users["home_country"].nunique())

#I've decided to keep Home Country

df_users["home_country"] = (df_users["home_country"] == 'usa').astype("int")

df_users.head()

# check
df_users.dtypes

# PCA cannot work with NaN values therefore I Replaced NaN with 0 where possible

print(df_users.isnull().sum())

#drop gender NaN
#df_users["gender"].dropna(inplace=True)

df_users.fillna(0, inplace=True)

# Before apply PCA, check if there is null in database
print(df_users.isnull().sum())

"""# Data scaling(normalizing numerical features)"""

# Data scaling(normalizing numerical features)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_users)
customers_scaled = pd.DataFrame(X_scaled)
customers_scaled.index = df_users.index
customers_scaled.columns = df_users.columns

# check/print scaled data columns
customers_scaled.head(2)

"""# I want to preserve 95% of the data variance, so I have to have enough pca components. The number of components we need for this should be determined by Python."""

# We want to be able to preserve 95% of the data variance, so we have to have enough pca components

#The number of components we need for this should be determined by Python.

var_exp = 0.95
pca = PCA(n_components = var_exp, random_state= 42)
customers_pca = pca.fit_transform(customers_scaled)


customers_pca = pd.DataFrame(customers_pca)
customers_pca.index = customers_scaled.index
customers_pca.columns = [f"pca_{i}"for i in range(customers_pca.shape[1])]

customers_pca.to_csv(f'{directory}/user_pca.csv')
print(customers_pca.shape)
customers_pca.head()

"""# # Display eigenvalues ​​- coefficients that are crucial for every PC"""

# # Display eigenvalues ​​- coefficients that are crucial for every PC
component_matrix = pd.DataFrame(pca.components_).T
component_matrix.columns = [f"pca_{i}"for i in range(component_matrix.shape[1])]
component_matrix.index = customers_scaled.columns
component_matrix

"""# in this step we perform PCA. n_commponents = None in order to calculate the variance of all possible principal components."""

# in this step we perform PCA. n_commponents = None in order to
# calculate the variance of all possible principal components.
pca = PCA(n_components=None, random_state= 42)
pca.fit(X_scaled)

# Then Calculate Cumulative Variance
# The 'explained_variance_ratio_' indicates the percentage of the variance that is explained by each principal component.

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# display graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')
plt.title('Cumulative variance explained by the principal components')
plt.xlabel('Number of principle components')
plt.ylabel('Cumulative explained variance (%)')
plt.grid(True)
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variace thresholds ')

# we have to find the point at which the cumulative variance exceeds 95%
n_components_95 = np.where(cumulative_variance >= var_exp)[0][0] + 1
plt.axvline(x=n_components_95, color='g', linestyle='--', label=f'{n_components_95} Komponenten für {var_exp * 100}%')
plt.text(n_components_95, 0.5, f'{n_components_95} Components', color='g', ha='right', va='center')

plt.legend(loc='lower right')
plt.tight_layout()
plt.show()

cumulative_variance

"""Now let's look at which combination of variables contributes most to our component value. This will help us understand our components a little better.

This PCA heatmap shows that most principal components are driven by a small number of strongly loading features, indicating that the model has learned several distinct and interpretable behavioral dimensions rather than diffuse combinations of all variables. In particular, clusters of high loadings appear around logically related groups such as flight activity and spending, hotel and accommodation behavior, online engagement (sessions, clicks, duration), seasonality of travel, and group or family travel characteristics. This suggests that the underlying structure of the data is organized around clear travel‐behavior patterns rather than being dominated by demographic attributes.

The main takeaway is that behavioral features contribute far more to explaining variance than demographic indicators, which appear only in a limited number of components and with lower overall influence. Practically, this means the reduced PCA space captures meaningful traveler archetypes—such as frequent flyers, hotel-focused travelers, deal-seekers, or high-engagement users—which can be leveraged for segmentation, clustering, or downstream modeling. Overall, the heatmap supports that PCA has successfully compressed correlated variables into a smaller set of interpretable latent factors while preserving the most important information in the dataset.

The heatmap confirms that TravelTide users naturally separate into meaningful behavioral groups based on engagement, spending, trip composition, discount sensitivity, and life stage—making PCA + clustering an appropriate approach.
"""

from google.colab import files

# Save the DataFrame to the current working directory
customers_pca.to_csv("user_pca_analysis.csv", index=False)

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = 'user_pca_analysis.csv'

# Download the file
files.download(file_path)
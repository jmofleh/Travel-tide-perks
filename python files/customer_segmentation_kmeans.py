# -*- coding: utf-8 -*-
"""customer_segmentation_kmeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXRsaSjgul48eimNNBrCR14uGY5OHsO8
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Import needed libraries
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# connect directoy and data file
directory = "/content/drive/MyDrive/Travel tide project"

user_pca = pd.read_csv(f'{directory}/user_pca.csv', index_col=0)
user_pca.head()

user_pca.shape

print(user_pca.columns)

"""# Define the range for the number of clusters to be tested (e.g., 2 to 10)"""

# Define the range for the number of clusters to be tested (e.g., 2 to 10)
range_n_clusters = range(3, 11)
silhouette_scores = []
inertia_values = []

# Perform K-means for each k-value and calculate the silhouette score.
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(user_pca)
    silhouette_avg = silhouette_score(user_pca, cluster_labels)
    silhouette_scores.append(silhouette_avg)
    inertia_values.append(kmeans.inertia_)

# Find best silhouette k
optimal_n_clusters = range_n_clusters[np.argmax(silhouette_scores)]

# Create subplots
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14, 5))

#  Create Elbow Plot
ax1.plot(range_n_clusters, inertia_values, marker='o')
ax1.set_title("Elbow Method")
ax1.set_xlabel("Number of Clusters (k)")
ax1.set_ylabel("Inertia")
ax1.grid(True)

# --- Silhouette Plot ---
ax2.plot(range_n_clusters, silhouette_scores, marker='o')
ax2.axvline(optimal_n_clusters, color='red', linestyle='--', label=f'Optimal k = {optimal_n_clusters}')
ax2.set_title("Silhouette Analysis")
ax2.set_xlabel("Number of Clusters (k)")
ax2.set_ylabel("Silhouette Score")
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

print(f"Optimal number of clusters based on Silhouette: {optimal_n_clusters}")

#Usually we choose value of K (number of clusters) then we evaluate this number of with Silhouette
#Analysis  and the K that gives the highes silhouette score issnually considere the best k but in our
#case however number cluster for silhouette analysis is 3, because we have five perks and also
#with Elbow method suggest 5-6 clusters so optional n =5
optimal_n_clusters = 5

"""# K-mean clustering with five clusters"""

from matplotlib.colors import ListedColormap

# Custom colors for clusters
custom_colors = [
    "#FDE68A",  # light yellow (group 0)
    "#A7F3D0",  # light green  (group 1)
    "#60A5FA",  # blue
    "#F87171",  # red
    "#A78BFA"   # purple
]

cmap = ListedColormap(custom_colors)

# Perform K-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)
user_pca['group'] = kmeans.fit_predict(user_pca)

plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    user_pca.iloc[:, 0],
    user_pca.iloc[:, 1],
    c=user_pca['group'],
    cmap=cmap,
    s=50,
    alpha=0.7
)

plt.title(f'K-Means Clustering with {optimal_n_clusters} Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.grid(True)

# Add legend
legend = plt.legend(*scatter.legend_elements(), title="Groups")
plt.show()

cluster_profiles = user_pca.groupby('group').mean()
display(cluster_profiles.round(2))

"""This K-means clustering plot shows that most of the separation between the five clusters occurs along PCA Component 1, indicating that the primary dimension driving segmentation is captured on the horizontal axis. One cluster (red) is clearly separated on the right with high PC1 values, suggesting a distinct group with strong underlying behavioral characteristics relative to others. On the left side, another cluster (green) occupies low PC1 values, forming a contrasting group. The remaining clusters (blue, purple, yellow) are more concentrated around the center and partially overlap, implying they share some similarities but still differ in subtler ways captured by the combination of PCA Component 1 and 2.

The key takeaway is that the data contains one or two strongly distinct traveler segments, plus several moderately differentiated groups clustered around average behavior. PCA Component 2 mainly adds vertical spread but does not create strong horizontal separations, meaning it refines distinctions rather than defining primary segments. Overall, the clustering appears reasonable: there is meaningful structure, but not extreme separation, which is typical for real behavioral data. These clusters can now be interpreted by examining feature averages per cluster to assign practical labels such as high-activity travelers, low-engagement users, deal-seekers, or group-oriented travelers
"""

#save the user_id -> group connection in a csv

user_pca['group'].to_csv(f'{directory}/user_segment.csv')
user_pca['group']

# count of users per cluster
clusters = KMeans(n_clusters=optimal_n_clusters, random_state=42)\
            .fit_predict(user_pca.select_dtypes(include=["int64","float64"]))

output_df = user_pca.assign(cluster=clusters).reset_index()
output_df.to_csv("user_base_with_clusters.csv", index=False)

print("Clustering completed.")
print(output_df.cluster.value_counts())

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = f'{directory}/user_segment.csv'

# Download the file
files.download(file_path)
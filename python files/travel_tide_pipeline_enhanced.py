
"""
TRAVELTIDE END-TO-END ANALYTICS PIPELINE
---------------------------------------
Stages:
1. Data Preprocessing & EDA
2. Trip & User Level Feature Engineering
3. User PCA
4. Customer Segmentation (KMeans)
5. Cluster Profiling

Run this file to execute full pipeline.
"""

import pandas as pd
import numpy as np

# ============================
# MAIN EXECUTION CONTROLLER
# ============================

def main():
    print("Starting TravelTide Pipeline...")

    # --- Data Preprocessing ---
    print("Step 1: Data Processing")
    # (code below)

    # --- Feature Engineering ---
    print("Step 2: Trip & User Level Features")

    # --- PCA ---
    print("Step 3: PCA")

    # --- Clustering ---
    print("Step 4: KMeans")

    # --- Profiling ---
    print("Step 5: Cluster Profiling")

    print("Pipeline Complete.")


# ============================
# PIPELINE CODE
# ============================

# TRAVELTIDE END-TO-END PIPELINE


# =====================
# data_preprocessing_and_eda.py
# =====================
# -*- coding: utf-8 -*-
"""data_preprocessing_and_eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PUj0he3a1p4NpSB9dWiJNdBYztwkCw1i
"""

# import pandas
import pandas as pd

#Connect with google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Connect with directory and data file, read number of columns and record
directory = "/content/drive/MyDrive/Travel tide project"

df_session_base = pd.read_csv(directory + "/travel_tide.csv")
#df_session_base = pd.read_csv(directory + "/not_canceled_trips.csv")


print(df_session_base.shape)
display(df_session_base.head())

"""In the next cell two data frames will be created, data types will be standardized  and several  coulumns will be converted  from text into proper datetime format in both DataFrames so that time-based calculations and analysis can be performed correctly

"""

# two data frames: data type standardization and date conversion
df_trips = df_session_base.convert_dtypes()
df_session_base = df_session_base.convert_dtypes()

cols = ['session_start', 'session_end', 'birthdate', 'sign_up_date', 'departure_time', 'return_time', 'check_in_time', 'check_out_time']

for c in cols:
  df_trips[c] = pd.to_datetime(df_trips[c], format='mixed')
  df_session_base[c] = pd.to_datetime(df_session_base[c], format='mixed')

"""I converted selected timestamp columns into proper datetime format and then calculated the length of each session in seconds by subtracting start time from end time."""

datetime_cols = ['session_start', 'session_end', 'departure_time', 'return_time']

for col in datetime_cols:
    df_session_base[col] = pd.to_datetime(df_session_base[col], format='mixed')

df_session_base['session_duration'] = (df_session_base['session_end'] - df_session_base['session_start']).dt.total_seconds()
display(df_session_base[['session_start', 'session_end', 'session_duration']].head(1))

"""# To read number of null in different columns"""

df_session_base.isnull().sum()

"""# Show the summary statisstics of df session (transposed)"""

# Show the summary statisstics of df session (transposed)
df_session_base.describe().T.round(2)

"""I filtered the dataset to find trips with zero or negative nights, selected relevant columns, and displayed two sample rows for inspection."""

df_session_base[df_session_base['nights'] <= 0][['trip_id', 'check_in_time', 'check_out_time', 'nights']].head(2)

"""# Convert 'birthdate' to datetime objects  (Request of learning Journey)"""

# Convert 'birthdate' to datetime objects  (Request of learning Journey)
df_session_base['birthdate'] = pd.to_datetime(df_session_base['birthdate'])

# Display the earliest and latest birthdates
print(f"Earliest birthdate: {df_session_base['birthdate'].min()}")
print(f"Latest birthdate: {df_session_base['birthdate'].max()}")

# Extract birth year( Request of learning Journey)
df_session_base['birth_year'] = df_session_base['birthdate'].dt.year

# Get the current year to calculate age
current_year = pd.to_datetime('today').year

# Calculate age
df_session_base['age'] = current_year - df_session_base['birth_year']

# Display summary statistics for age
print("Summary statistics for Age:" )
display((df_session_base['age'].describe().round(1).to_frame()
    .T))

# Display the distribution of birth years(Request of learning Journey))
print("Top 10 most common birth years:")
display(df_session_base['birth_year'].value_counts().head(10))

"""I filtered the dataset to show sessions with at least 100 page clicks and displayed key columns to analyze high-engagement user behavior."""

# high engagement behaviour(?)
df_session_base[df_session_base['page_clicks'] >= 100][['user_id', 'trip_id', 'hotel_booked', 'flight_booked', 'session_duration', 'page_clicks']]

"""# New column Marriage status based on the 'married' column (EDA)"""

# New column Marriage status based on the 'married' column (EDA)
df_session_base['marriage_status'] = df_session_base['married'].map({True: 'Married', False: 'Not Married'})

print("Marriage Status Distribution:")
display(df_session_base['marriage_status'].value_counts())

"""# EDA graphs"""

#EDA graphs
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = df_session_base.copy()

light_blue = "#ADD8E6"   # Light blue color

sns.set_style("whitegrid")
plt.style.use("ggplot")

print("\n--- Generating Visualizations for df_session ---")

# --- Figure 1: Distribution plots ---
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
fig.suptitle('Distribution of Session Metrics', fontsize=16)

sns.histplot(df['page_clicks'], bins=20, kde=True, color="#5DADE2", ax=axes[0, 0])
axes[0, 0].set_title('Distribution of Page Clicks')

sns.histplot(df['nights'], bins=10, kde=False, color=light_blue, ax=axes[0, 1])
axes[0, 1].set_title('Distribution of Nights Booked')

sns.histplot(df['base_fare_usd'], bins=20, kde=True, color= light_blue, ax=axes[1, 0])
axes[1, 0].set_title('Distribution of Base Fare (USD)')

sns.histplot(df['hotel_price_per_room_night_usd'], bins=20, kde=True, color="#AF7AC5", ax=axes[1, 1])
axes[1, 1].set_title('Distribution of Hotel Price per Room Night (USD)')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()


# --- Figure 2: Categorical count plots ---
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))
fig.suptitle('Booking and Discount Statuses', fontsize=16)

booked_df = df[['flight_booked', 'hotel_booked']].melt(
    var_name='Booking Type', value_name='Booked')
sns.countplot(x='Booking Type', hue='Booked', data=booked_df, ax=axes[0], palette="Blues")
axes[0].set_title('Flight vs. Hotel Booked')
axes[0].set_xlabel('Booking Type')
axes[0].set_ylabel('Count')

# Count plot for flight and hotel discounts
discount_df = df[['flight_discount', 'hotel_discount']].melt(var_name='Discount Type', value_name='Applied')
sns.countplot(x='Discount Type', hue='Applied', data=discount_df,palette="Blues", ax=axes[1])
axes[1].set_title('Flight vs. Hotel Discount Applied')
axes[1].set_xlabel('Discount Type')
axes[1].set_ylabel('Count')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Figure 3: Relationship plots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))
fig.suptitle('Relationships in Session Data', fontsize=16)

# Scatter plot: Page clicks vs. Base fare
sns.scatterplot(x='page_clicks', y='base_fare_usd', data=df, ax=axes[0])
axes[0].set_title('Page Clicks vs. Base Fare (USD)')
axes[0].set_xlabel('Page Clicks')
axes[0].set_ylabel('Base Fare (USD)')

# Box plot: Base fare by flight booked
sns.boxplot(
    x='flight_booked',
    y='base_fare_usd',
    hue='flight_booked',
    data=df,
    palette="Blues",
    ax=axes[1],
    showfliers=False,
    legend=False
)

axes[1].set_title('Base Fare by Flight Booked (Outliers Removed)')
axes[1].set_xlabel('Flight Booked')
axes[1].set_ylabel('Base Fare (USD)')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""# Histogram session duration"""

# Historgraph/plot session duration
def plot_distribution(df, column_name):

  import matplotlib.pyplot as plt
  import seaborn as sns
  fig, axes = plt.subplots(1, 2, figsize=(12, 6))

  # Boxplot
  sns.boxplot(y=df[column_name], ax=axes[0])
  axes[0].set_title(f'Boxplot of {column_name}')

  # Histogram
  sns.histplot(df[column_name], ax=axes[1], kde=True)
  axes[1].set_title(f'Histogram of {column_name}')

  plt.tight_layout()
  plt.show()
  import matplotlib.pyplot as plt
plot_distribution(df_session_base, 'session_duration')

"""#Flight destination by volumn(number of flights per destination -map)"""

#Flight destination by volumn(map)

df = df[["destination_airport_lon", "destination_airport_lat", 'destination']].copy()
df.dropna(inplace=True)

print(df.shape)

import plotly.express as px
import pandas as pd

#  Aggregate the data: Count flights per destination

map_df = df.groupby(['destination', 'destination_airport_lat', 'destination_airport_lon']).size().reset_index(name='flight_count')


fig = px.scatter_geo(
    map_df,
    lat='destination_airport_lat',
    lon='destination_airport_lon',
    size='flight_count',
    hover_name='destination',
    projection="natural earth",
    title='Flight Destinations by Volume',
    template='plotly_white'
)

fig.update_traces(marker=dict(color='Green', line=dict(width=2, color='Blue')))


fig.show()

"""Handling outliers in two methods, removing the outliers and clipping of outliers"""

#Removes outliers from a specified column in a DataFrame using the IQR method, prepared for general use.
def remove_outliers_iqr(df, column_name):

  Q1 = df[column_name].quantile(0.25)
  Q3 = df[column_name].quantile(0.75)
  IQR = Q3 - Q1

  lower_bound = Q1 - 5 * IQR
  upper_bound = Q3 + 5 * IQR

  original_rows = df.shape[0]
  df_cleaned = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)].copy()
  removed_rows = original_rows - df_cleaned.shape[0]

  print(f"Removed {removed_rows} outliers from the '{column_name}' column.")

  return df_cleaned

# Outlier removed from session duration and graph
df_cleaned = remove_outliers_iqr(df_session_base, 'session_duration')
plot_distribution(df_cleaned, 'session_duration')

#clip outliers
def clip_outliers_iqr(df, column_name):
  Q1 = df[column_name].quantile(0.25)
  Q3 = df[column_name].quantile(0.75)
  IQR = Q3 - Q1

  # Using 5 * IQR for consistency with the remove_outliers_iqr function
  lower_bound = Q1 - 5 * IQR
  upper_bound = Q3 + 5 * IQR

  df_clipped = df.copy()
  df_clipped[column_name] = df_clipped[column_name].clip(lower=lower_bound, upper=upper_bound)

  print(f"Clipped outliers in the '{column_name}' column using IQR method.")
  return df_clipped

df_session_clip = clip_outliers_iqr(df_session_base, "page_clicks")
df_session_clip = clip_outliers_iqr(df_session_clip, "session_duration")
#remove outliers
df_session_rm = remove_outliers_iqr(df_session_base, "page_clicks")
df_session_rm = remove_outliers_iqr(df_session_rm, "session_duration")

plot_distribution(df_session_clip, 'page_clicks')
plot_distribution(df_session_base, "nights")

"""#Number of nights (Hotel) -2 and-1 nights changed to Zero"""

#Number of nights (Hotel)
import pandas as pd

nights_distribution = (
    df_session_base.groupby("nights")
          .size()
          .reset_index(name="rows")
          .sort_values("nights")
)
df_session_base.loc[df_session_base["nights"] < 0, "nights"] = 0 #-2 and -1 nights changed to zero.
print(nights_distribution)

"""#  if trip includes a flight, a hotel, both, or neither, and counts how many trips fall into each category."""

# How many trips had flights and hotels or any of them
trip_summary = df_session_base.groupby("trip_id").agg(
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

print(trip_summary[["has_flight", "has_hotel"]].value_counts())

#I filtered trip rows, cleaned numeric data, calculated per-row spending,
# and aggregated everything into a clean trip-level dataset with total costs per trip.

df_trip = df_session_base[df_session_base["trip_id"].notna()].copy()

# Convert numeric columns safely
num_cols = ["seats", "base_fare_usd", "nights", "rooms", "hotel_price_per_room_night_usd"]
for c in num_cols:
    df_trip[c] = pd.to_numeric(df_trip[c], errors="coerce").fillna(0)

# Clean invalid nights
df_trip.loc[df_trip["nights"] <= 0, "nights"] = 0

# Spend per row
df_trip["flight_spend_row"] = df_trip["seats"] * df_trip["base_fare_usd"]
df_trip["hotel_spend_row"] = (
    df_trip["nights"] * df_trip["rooms"] * df_trip["hotel_price_per_room_night_usd"]
)
df_trip["total_spend_row"] = df_trip["flight_spend_row"] + df_trip["hotel_spend_row"]

# Aggregate to trip-level
trip_table = (
    df_trip.groupby("trip_id", as_index=False)
    .agg(
        user_id=("user_id", "first"),
        destination=("destination", "first"),
        trip_airline=("trip_airline", "first"),
        origin_airport=("origin_airport", "first"),
        destination_airport=("destination_airport", "first"),
        departure_time=("departure_time", "first"),
        return_time=("return_time", "first"),

        hotel_name=("hotel_name", "first"),
        total_nights=("nights", "sum"),
        total_rooms=("rooms", "sum"),

        flight_spend=("flight_spend_row", "sum"),
        hotel_spend=("hotel_spend_row", "sum"),
        total_spend=("total_spend_row", "sum")
    )
)

trip_table.head(2)

"""#Aggreates of nights, rooms, flight and hotel spending at user ID level"""

#Aggreates of nights, rooms, flight and hotel spending at user ID level
user_table = (
    trip_table
    .groupby("user_id", as_index=False)
    .agg(
        total_nights=("total_nights", "sum"),
        total_rooms=("total_rooms", "sum"),
        total_flight_spend=("flight_spend", "sum"),
        total_hotel_spend=("hotel_spend", "sum"),
        total_spend=("total_spend", "sum")
    )
)

user_table.head()

"""# Finding canceled and not canceled trips"""

#find canceled sessions from SQL database
import pandas as pd
from sqlalchemy import create_engine

# Define your PostgreSQL connection parameters
username = 'Test'
password = 'bQNxVzJL4g6u'
host = 'ep-noisy-flower-846766.us-east-2.aws.neon.tech'        # or your DB host
port = '5432'             # default PostgreSQL port
database = 'TravelTide'

# Create connection string using SQLAlchemy
conn_str = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'
engine = create_engine(conn_str)

query = '''
SELECT DISTINCT trip_id
FROM sessions
WHERE cancellation = TRUE
'''

canceled_trip_ids = set(pd.read_sql(query, engine)['trip_id'].values)
print('Number of canceled trips (total)', len(canceled_trip_ids))
engine.dispose()

# counts rows per cancellation status
df_session_base.groupby("cancellation").size().reset_index(name="count")

# Breakdown of Cancelled Trips by Booking Type
cancelled_trips_df = df_session_base[df_session_base['cancellation'] == True]

# Get unique cancelled trip IDs
unique_cancelled_trip_ids = cancelled_trips_df['trip_id'].unique()

# Define trip_summary
trip_summary = df_trip.groupby("trip_id").agg(
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

# Filter trip_summary to get details for only cancelled trips
cancelled_trip_details = trip_summary[trip_summary['trip_id'].isin(unique_cancelled_trip_ids)]


# Count the occurrences of each combination
cancellation_type_counts = cancelled_trip_details[['has_flight', 'has_hotel']].value_counts().reset_index(name='count')
cancellation_type_counts['booking_type'] = cancellation_type_counts.apply(
    lambda row: 'Flight and Hotel' if row['has_flight'] and row['has_hotel']
    else ('Flight Only' if row['has_flight']
    else ('Hotel Only' if row['has_hotel']
    else 'Unknown/Other')),
    axis=1
)

print("Breakdown of Cancelled Trips by Booking Type:")
display(cancellation_type_counts[['booking_type', 'count']])

#Only non-canceled trips and printed how many rows (sessions) those trips have.
df_temp = df_session_base.dropna(subset = ['trip_id'])
df_not_canceled_trips = df_temp[~df_temp['trip_id'].isin(canceled_trip_ids)]
print(df_not_canceled_trips.shape[0])

# check number of null in df
print(df_not_canceled_trips.isnull().sum())

"""#I cleaned trip IDs, counted sessions, trips, and canceled trips per user,and merged everything into a single user-level summary table"""

#I cleaned trip IDs, counted sessions, trips, and canceled trips per user,
# and merged everything into a single user-level summary table.

# Ensure column cleanliness if needed
df_session_base["trip_id"] = df_session_base["trip_id"].astype(str).str.strip()
df_session_base["trip_id"] = df_session_base["trip_id"].replace(["", "nan", "None", "null"], pd.NA)

# 1) Number of sessions per user
sessions_per_user = df_session_base.groupby("user_id")["session_id"].nunique().reset_index(name="num_sessions")

# 2) Number of trips per user (trip_id not null)
trips_per_user = (
    df_session_base[df_session_base["trip_id"].notna()]
    .groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_trips")
)

# 3) Count canceled trips per user using the 'cancellation' column
# Filter for actual canceled entries where trip_id is also present
canceled_df = df_session_base[(df_session_base['cancellation'] == True) & (df_session_base['trip_id'].notna())]

canceled_trips_per_user = (
    canceled_df.groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_canceled_trips")
)

# 4) Merge into one final table
user_summary = (
    sessions_per_user
    .merge(trips_per_user, on="user_id", how="left")
    .merge(canceled_trips_per_user, on="user_id", how="left")
)

# Fill missing (users with 0 trips or 0 canceled trips)
user_summary["num_trips"] = user_summary["num_trips"].fillna(0).astype(int)
user_summary["num_canceled_trips"] = user_summary["num_canceled_trips"].fillna(0).astype(int)

user_summary.head()



"""# I built user-level features counting how many trips each user canceled and how many trips they had in total."""

df_user_base1 = canceled_df.groupby('user_id').agg(
 num_canceled_trips = ('trip_id', 'nunique')
).reset_index()

df_user_base2 = df_session_base[df_session_base['trip_id'].notna()].groupby('user_id').agg(
 num_trips = ('trip_id', 'nunique')
).reset_index()

df_user_base1= pd.merge(df_user_base1, df_user_base2, on='user_id', how='left')
df_user_base1.head()

"""### Comparing Number of Trips to Cancelled Trips

To understand the relationship between the total number of trips and cancelled trips, we can calculate a cancellation rate per user. This metric will show what percentage of a user's trips ended up being cancelled.
"""

# Calculate cancellation rate, handling division by zero
user_summary['cancellation_rate'] = user_summary.apply(
    lambda row: (row['num_canceled_trips'] / row['num_trips']) if row['num_trips'] > 0 else 0,
    axis=1
)


print("Descriptive statistics for cancellation rate:")
display(user_summary['cancellation_rate'].describe().round(3)),

print("\nTop 10 users by cancellation rate (excluding users with 0 trips initially, if desired):")
display(user_summary[user_summary['num_trips'] > 0].sort_values(by='cancellation_rate', ascending=False).head(10))


plt.show()



"""I counted the number of flights per booking and calculated the actual money spent per flight and hotel after applying any discounts. I also calculated the advance booking time in days by measuring the difference between booking completion and travel departure."""

# count number of Flights
import numpy as np
import pandas as pd

# Ensure boolean columns are treated as standard booleans (NA becomes False) for logical operations
flight_booked_bool = df_session_base['flight_booked'].fillna(False).astype(bool)
return_flight_booked_bool = df_session_base['return_flight_booked'].fillna(False).astype(bool)
flight_discount_bool = df_session_base['flight_discount'].fillna(False).astype(bool)
hotel_discount_bool = df_session_base['hotel_discount'].fillna(False).astype(bool)

# count number of Flights
df_session_base['num_flights'] = np.where(
    #if
    (flight_booked_bool == True) & (return_flight_booked_bool == True),
    2,
    #else
    np.where(
        (flight_booked_bool == True) & (return_flight_booked_bool == False),
        1,0
    )
)

df_session_base['money_spent_per_flight'] = np.where(
    (flight_discount_bool == True),
    df_session_base['base_fare_usd'] * (1 - df_session_base['flight_discount_amount']),
    df_session_base['base_fare_usd']
)

df_session_base['money_spent_per_seat'] = df_session_base['money_spent_per_flight'] / df_session_base['seats']

# money spent per hotel (total)
# rooms * nights * price_per_room_per_night
df_session_base['money_spent_total_hotel'] = df_session_base['rooms'] * df_session_base['nights'] * df_session_base['hotel_price_per_room_night_usd']

df_session_base['money_spent_per_hotel'] = np.where(
    (hotel_discount_bool == True),
    df_session_base['money_spent_total_hotel'] * (1 - df_session_base['hotel_discount_amount']),
    df_session_base['money_spent_total_hotel']
)

# Convert necessary columns to datetime
df_session_base['departure_time'] = pd.to_datetime(df_session_base['departure_time'], errors='coerce')
df_session_base['check_in_time'] = pd.to_datetime(df_session_base['check_in_time'], errors='coerce')
df_session_base['session_end'] = pd.to_datetime(df_session_base['session_end'], errors='coerce') # Added this line

# Time after booking
df_session_base['time_after_booking'] = np.where(
    (flight_booked_bool == True),
    (df_session_base['departure_time'] - df_session_base['session_end']).dt.days,
    (df_session_base['check_in_time'] - df_session_base['session_end']).dt.days
)
display(df_session_base[[
    'user_id',
    'flight_booked',
    'return_flight_booked',
    'num_flights',
    'money_spent_per_flight',
    'money_spent_per_seat',
    'money_spent_per_hotel',
    'time_after_booking'
]].head())

display(df.head(2))

df.to_csv(f'{directory}/session_base.csv', index=False)

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = f'{directory}//session_base.csv'

# Download the file
files.download(file_path)

# =====================
# trip_and_user_level_analysis.py
# =====================
# -*- coding: utf-8 -*-
"""trip_and_user_level_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWk7Bywq-FWrtie1AeU6VmUEwjzwMZzd
"""

# import libraries
import numpy as np
import pandas as pd

#connect to google drive
from google.colab import drive
drive.mount('/content/drive')

# connect to files/read files, shape and columns
directory = "/content/drive/MyDrive/Travel tide project"

df_session_base = pd.read_csv(directory + '//travel_tide.csv')
df_nc_sessions = pd.read_csv(directory + '/not_canceled_trips.csv')

print(df_session_base.shape)
print(df_nc_sessions.shape)

print(df_nc_sessions.columns)
df_session_base.head(2)

df_trips = df_nc_sessions.copy()

# I converted several timestamp columns into proper datetime format across three datasets so that time-based
# calculations and features can be created reliably.
cols = ['session_start', 'session_end', 'birthdate', 'sign_up_date', 'departure_time', 'return_time', 'check_in_time', 'check_out_time']

for c in cols:
  df_trips[c] = pd.to_datetime(df_trips[c], format='mixed')
  # df_session_base does not contain these timestamp columns, so this line is removed.
  # df_session_base[c] = pd.to_datetime(df_session_base[c], format='mixed')
  df_nc_sessions[c] = pd.to_datetime(df_nc_sessions[c], format='mixed')

"""# Feature Engineering for Session Base"""

# I created a flag for sessions without trips and then summarized session behavior per user
# to build a user-level engagement feature table.


# The original df_session_base (from session_base.csv) does not contain
# 'trip_id', 'session_start', 'session_end', or 'page_clicks' needed for these operations.
# df_trips (copy of df_nc_sessions) contains all these columns.
# For the purpose of this feature engineering block, I used will df_trips as the source.


df_session_base_for_feature = df_trips.copy()


#identify empty sessions
df_session_base_for_feature['empty_session'] = df_session_base_for_feature['trip_id'].isna().astype(int)

# Calculate session duration
df_session_base_for_feature['session_duration'] = (df_session_base_for_feature['session_end'] - df_session_base_for_feature['session_start']).dt.total_seconds() / 60

# aggregate session information per user
df_user_base = df_session_base_for_feature.groupby('user_id').agg(
 num_sessions = ('session_id', 'count'),
 avg_clicks_per_session = ('page_clicks', 'mean'),
 num_empty_sessions = ('empty_session', 'sum'),
 avg_session_duration = ('session_duration', 'mean'),
 std_session_duration = ('session_duration', 'std'),
  num_clicks= ('page_clicks', 'sum'),
    avg_session_clicks= ('page_clicks', 'mean')
).reset_index()

df_user_base.head(2).round(2)

# check
print(df_user_base.shape)
df_user_base.isnull().sum()

"""# Compare number of Trips with Canceled Trips"""

# counts rows per cancellation status
df_session_base.groupby("cancellation").size().reset_index(name="count")

"""# Breakdown of Cancelled Trips by Booking Type"""

# Breakdown of Cancelled Trips by Booking Type
cancelled_trips_df = df_session_base[df_session_base['cancellation'] == True]

# Get unique cancelled trip IDs
unique_cancelled_trip_ids = cancelled_trips_df['trip_id'].unique()

# Define trip_summary using df_session_base to cover all bookings
trip_summary = df_session_base.groupby("trip_id").agg(
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

# Filter trip_summary to get details for only cancelled trips
cancelled_trip_details = trip_summary[trip_summary['trip_id'].isin(unique_cancelled_trip_ids)]


# Count the occurrences of each combination
cancellation_type_counts = cancelled_trip_details[['has_flight', 'has_hotel']].value_counts().reset_index(name='count')
cancellation_type_counts['booking_type'] = cancellation_type_counts.apply(
    lambda row: 'Flight and Hotel' if row['has_flight'] and row['has_hotel']
    else ('Flight Only' if row['has_flight']
    else ('Hotel Only' if row['has_hotel']
    else 'Unknown/Other')),
    axis=1
)

print("Breakdown of Cancelled Trips by Booking Type:")
display(cancellation_type_counts[['booking_type', 'count']])

"""#Only non-canceled trips and printed how many rows (sessions) those trips have"""

#Only non-canceled trips and printed how many rows (sessions) those trips have.
df_temp = df_session_base.dropna(subset = ['trip_id'])

# Define canceled_trip_ids from df_session_base where cancellation is True
canceled_trip_ids = df_session_base[df_session_base['cancellation'] == True]['trip_id'].unique()

df_not_canceled_trips = df_temp[~df_temp['trip_id'].isin(canceled_trip_ids)]
print(df_not_canceled_trips.shape[0])

# Re-create df_user_base1 and df_user_base2 to ensure a clean state before merging

canceled_df = df_session_base[(df_session_base['cancellation'] == True) & (df_session_base['trip_id'].notna())]
df_user_base1 = canceled_df.groupby('user_id').agg(
    num_canceled_trips = ('trip_id', 'nunique')
).reset_index()
df_user_base2 = df_nc_sessions.groupby('user_id').agg(
    num_not_canceled_trips = ('trip_id', 'nunique')
).reset_index()

df_user_base1 = pd.merge(df_user_base1, df_user_base2, on='user_id', how='outer')
print(df_user_base1.shape)
df_user_base1.head()

##df_user_base1[(df_user_base1['num_canceled_trips'] > 0 ) & (df_user_base1['num_not_canceled_trips'] > 0)]

"""# Features and columns engineering"""

#check if df_trips is complete
assert df_nc_sessions.shape[0] == 15489

# I created new columns group_trip, pair_trip,business_week_trip, weekend_trip,discount_trip,season
def group_trip(row):
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and row['seats'] > 2 and row['rooms'] > 1:
    return 1
  #Bus/Bahn/Auto Reise
  elif row['flight_booked'] == False and row['hotel_booked'] == True and row['rooms'] > 1 :
    return 1
  else:
    return 0

def pair_trip(row):
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and not pd.isna(row['seats']) and not pd.isna(row['rooms']) and row['seats'] == 2 and row['rooms']==1:
    return 1
  else:
    return 0

def business_week_trip(row):
  birth_date = pd.to_datetime(row['birthdate'], format='mixed')
  age = (pd.Timestamp.today() - birth_date).days / 365
  departure = row['departure_time']
  return_ = row['return_time']
  #check for flights during the week
  ## Weekday: Monday = 0, Sunday = 6
  # Business days are 0 to 4 (Monday to Friday)
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and row['seats'] == 1 and row['nights'] >= 1 and row['nights'] < 6 and age >= 25 and age <= 60 and (departure.weekday() <= 4) and (return_.weekday() <= 4):
    return 1
  else:
    return 0

def weekend_trip(row):
  departure = row['departure_time']
  return_ = row['return_time']
  # Friday = 4, Sunday = 6
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and not pd.isna(row['nights']) and row['nights'] <= 2 and (departure.weekday() >= 4) and (return_.weekday() <= 6):
    return 1
  else:
    return 0

def season_trip(row):
  if row['departure_time'].month in [12, 1, 2]:
    return "winter"
  if row['departure_time'].month in [6, 7, 8]:
    return "summer"
  if row['departure_time'].month in [9, 10, 11]:
    return "fall"
  else:
    return "spring"

def discount_trip(row):
  if row['flight_discount'] == True or row['hotel_discount'] == True:
    return 1
  else:
    return 0

df_nc_sessions['group_trip'] = df_nc_sessions.apply(group_trip, axis = 1)
df_nc_sessions['pair_trip'] = df_nc_sessions.apply(pair_trip, axis = 1)
df_nc_sessions['business_week_trip'] = df_nc_sessions.apply(business_week_trip, axis = 1)
df_nc_sessions['weekend_trip'] = df_nc_sessions.apply(weekend_trip, axis = 1)
df_nc_sessions['discount_trip'] = df_nc_sessions.apply(discount_trip, axis = 1)
df_nc_sessions['season'] = df_nc_sessions.apply(season_trip, axis = 1)

# I created multiple new features measuring flight count, flight and hotel spending, and booking lead time to better
#understand trip purchasing behavior
df_trips['num_flights'] = np.where(
    #if
    (df_trips['flight_booked'] == True) & (df_trips['return_flight_booked'] == True),
    2,
    #else
    np.where(
        (df_trips['flight_booked'] == True) & (df_trips['return_flight_booked'] == False),
        1,0
    )
)

df_trips['money_spent_per_flight'] = np.where(
    (df_trips['flight_discount'] == True),
    df_trips['base_fare_usd'] * (1 - df_trips['flight_discount_amount']),
    df_trips['base_fare_usd']
)

df_trips['money_spent_per_seat'] = df_trips['money_spent_per_flight'] / df_trips['seats']

# money spent per hotel (total)
# rooms * nights * price_per_room_per_night
df_trips['money_spent_total_hotel'] = df_trips['rooms'] * df_trips['nights'] * df_trips['hotel_price_per_room_night_usd']

df_trips['money_spent_per_hotel'] = np.where(
    (df_trips['hotel_discount'] == True),
    df_trips['money_spent_total_hotel'] * (1 - df_trips['hotel_discount_amount']),
    df_trips['money_spent_total_hotel']
)

# Ensure relevant columns are datetime objects
df_trips['departure_time'] = pd.to_datetime(df_trips['departure_time'], errors='coerce')
df_trips['session_end'] = pd.to_datetime(df_trips['session_end'], errors='coerce')
df_trips['check_in_time'] = pd.to_datetime(df_trips['check_in_time'], errors='coerce')

# Time after booking
df_trips['time_after_booking'] = np.where(
    (df_trips['flight_booked'] == True),
    (df_trips['departure_time'] - df_trips['session_end']).dt.days,
    (df_trips['check_in_time'] - df_trips['session_end']).dt.days
)
df_trips[['user_id', 'money_spent_total_hotel', 'departure_time', 'time_after_booking']].head()

# I engineered a binary feature identifying group travelers (family and friends) and a categorical feature labeling the season in which each trip occurred.
df_trips['family_and_friends'] = np.where(
    ((df_trips['flight_booked'] == True) & (df_trips['seats'] > 1)),
    1,0
)

# Give back the name of the season in which the customer traveled.
#return 'winter' if trip happened in winter
def season_helper(row):
  #define time
  time = row['departure_time'] if row['flight_booked'] == True else row['check_in_time']

  # Ensure time is a datetime object or NaT and handle NaT values
  if pd.isna(time):
      return None # Or 'unknown', or a placeholder for missing dates

  month = time.month
  if month in [12, 1, 2]:
    return 'winter'
  elif month in [3, 4, 5]:
    return 'spring'
  elif month in [6, 7, 8]:
    return 'summer'
  else:
    return 'fall'


df_trips['season'] = df_trips.apply(season_helper, axis=1)

df_trips[['user_id', 'season', 'family_and_friends', 'trip_id']].head()

#Check if df_trips is complete
assert df_trips.shape[0] == 15489

"""# categorical season column converted into multiple binary numeric columns using one-hot encoding and appended them to your dataset"""

# categorical season column converted into multiple binary numeric columns using one-hot encoding and appended them to your dataset
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False)
season_encoded = encoder.fit_transform(df_trips[['season']])
df_season_encoded = pd.DataFrame(season_encoded, columns=encoder.get_feature_names_out(['season']))
df_trips = pd.concat([df_trips, df_season_encoded], axis=1)

#to confirm that df contains exactly 15,489 rows, ensuring data integrity.
assert df_trips.shape[0] == 15489

"""# Fly distance calculation Haversine distance"""

# Fly distance calculation Haversine distance
import math
def haversine_distance(lat1, lon1, lat2, lon2):
  if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):
    return np.nan
  # Convert latitude and longitude from degrees to radians
  R = 6371
  lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
  dlon = lon2 - lon1
  dlat = lat2 - lat1

  a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2
  c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
  distance = R * c
  return distance

# I created a new feature that calculates the kilometer distance between home
#and destination airports for each trip using geographic coordinates.
df_trips['distance_km'] = df_trips.apply(lambda row: haversine_distance(row['home_airport_lat'], row['home_airport_lon'],
                                                                        row['destination_airport_lat'], row['destination_airport_lon']), axis= 1)

#calculated money spent per flight KM
df_trips['money_spent_per_km'] = df_trips['money_spent_per_flight'] / df_trips['distance_km']

df_trips.columns

"""# Before aggregation, ensure unique columns to avoid AttributeError Keep the first occurrence of duplicate columns"""

# Before aggregation, ensure unique columns to avoid AttributeError
# Keep the first occurrence of duplicate columns
df_trips_cleaned = df_trips.loc[:, ~df_trips.columns.duplicated()]

# Features
df_user_base2 = df_trips_cleaned.groupby('user_id').agg(
 num_flights = ('num_flights', 'sum'),
 num_hotels = ('hotel_booked', 'sum'),
 avg_bags = ('checked_bags', 'mean'),
 num_flight_discounts = ('flight_discount', 'sum'),
 num_hotel_discounts = ('hotel_discount', 'sum'),
 avg_night = ('nights', 'mean'),
 avg_rooms = ('rooms', 'mean'),
 avg_seats = ('seats', 'mean'),
 avg_money_spent_per_flight = ('money_spent_per_flight', 'mean'),
 avg_money_spent_per_seat = ('money_spent_per_seat', 'mean'),
 avg_money_spent_per_hotel = ('money_spent_per_hotel', 'mean'),
 avg_time_after_booking = ('time_after_booking', 'mean'),
 num_fam_and_fri_trips = ('family_and_friends', 'sum'),
 num_winter = ('season_winter', 'sum'),
 num_spring = ('season_spring', 'sum'),
 num_summer = ('season_summer', 'sum'),
 num_fall = ('season_fall', 'sum'),
 avg_flight_km = ('distance_km', 'mean'),
 avg_money_spent_per_km = ('money_spent_per_km', 'mean')
).reset_index()
df_user_base2.head().round(2)

# to check number of nulls in df
print(df_user_base2.shape)
df_user_base2.isna().sum()

#missing-value cleanup
df_user_base2.fillna(0, inplace=True)

"""# Static User Features"""

#Importing database from SQL
from sqlalchemy import create_engine
import sqlalchemy as sa

# Define your PostgreSQL connection parameters
username = 'Test'
password = 'bQNxVzJL4g6u'
host = 'ep-noisy-flower-846766.us-east-2.aws.neon.tech'
port = '5432'             # default PostgreSQL port
database = 'TravelTide'

# Create connection string using SQLAlchemy
conn_str = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'
engine = create_engine(conn_str)

users = pd.read_sql("SELECT * FROM users", engine)
users.head(2)

# converted birthdates to datetime, computed user ages, and created a demographic user table with
# key personal attributes.
users['birthdate'] = pd.to_datetime(users['birthdate'], format='mixed')
users['age'] = (pd.to_datetime('today') - users['birthdate']).dt.days // 365

df_user_base3 = users[['user_id', 'gender', 'married', 'has_children', 'home_country', 'age']].copy()
df_user_base3.head(3)

"""Three indices calculated:
1- Flight Hunter index
2- Hotel Hunter Index
3-Bundle lover index
"""

import pandas as pd
import numpy as np


# Ensure trip_id is clean for processing as it's used in user_summary and trip_summary creation
df_session_base["trip_id"] = df_session_base["trip_id"].astype(str).str.strip().replace(["", "nan", "None", "null"], pd.NA)

# Re-create user_summary to get 'num_trips' and other user-level metrics
sessions_per_user = df_session_base.groupby("user_id")["session_id"].nunique().reset_index(name="num_sessions")
trips_per_user = (
    df_session_base[df_session_base["trip_id"].notna()]
    .groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_trips")
)
canceled_df = df_session_base[(df_session_base['cancellation'] == True) & (df_session_base['trip_id'].notna())]
canceled_trips_per_user = (
    canceled_df.groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_canceled_trips")
)
user_summary = (
    sessions_per_user
    .merge(trips_per_user, on="user_id", how="left")
    .merge(canceled_trips_per_user, on="user_id", how="left")
)
user_summary["num_trips"] = user_summary["num_trips"].fillna(0).astype(int)
user_summary["num_canceled_trips"] = user_summary["num_canceled_trips"].fillna(0).astype(int)


# Re-create df_trip for `trip_summary` (from previous cell Xz7rumqlbk0q's logic)
df_trip = df_session_base[df_session_base["trip_id"].notna()].copy()
num_cols_df_trip = ["seats", "base_fare_usd", "nights", "rooms", "hotel_price_per_room_night_usd"]
for c in num_cols_df_trip:
    df_trip[c] = pd.to_numeric(df_trip[c], errors="coerce").fillna(0)
df_trip.loc[df_trip["nights"] <= 0, "nights"] = 0

trip_summary = df_trip.groupby("trip_id").agg(
    user_id=("user_id", "first"), # Get user_id for merging later
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

# Create an 'is_bundle' column in trip_summary first
trip_summary['is_bundle'] = trip_summary['has_flight'] & trip_summary['has_hotel']

# Calculate user-level trip type counts from trip_summary
user_trip_types = trip_summary.groupby('user_id').agg(
    num_flight_trips=('has_flight', lambda x: (x == True).sum()),
    num_hotel_trips=('has_hotel', lambda x: (x == True).sum()),
    num_bundle_trips=('is_bundle', lambda x: (x == True).sum()) # Use the new 'is_bundle' column
).reset_index()


# Merge user-level trip counts into user_summary
user_summary = user_summary.merge(user_trip_types, on='user_id', how='left')

# Fill NaNs that might result if a user has no flights/hotels/bundles
user_summary[["num_flight_trips", "num_hotel_trips", "num_bundle_trips"]] = \
    user_summary[["num_flight_trips", "num_hotel_trips", "num_bundle_trips"]].fillna(0).astype(int)


#  Merge these user-level aggregates into df_session_base
# Prepare the columns before merging to avoid KeyError later
cols_to_merge_from_user_summary = ['num_sessions', 'num_trips', 'num_canceled_trips',
                                   'num_flight_trips', 'num_hotel_trips', 'num_bundle_trips']

user_summary_to_merge = user_summary[['user_id'] + cols_to_merge_from_user_summary].copy()
# Fill NaNs and convert type in the temporary DataFrame before merge
for col in cols_to_merge_from_user_summary:
    user_summary_to_merge[col] = user_summary_to_merge[col].fillna(0).astype(int)

df_session_base = df_session_base.merge(user_summary_to_merge, on='user_id', how='left')

# The fillna and astype for these specific columns on df_session_base is now done via the user_summary_to_merge


# ensure coluumns are numeric
cols = ["num_trips", "num_hotel_trips", "num_flight_trips", "num_bundle_trips"]
for c in cols:
    df_session_base[c] = pd.to_numeric(df_session_base[c], errors="coerce").fillna(0)

# Hotel Hunter Index
df_session_base["hotel_hunter_index"] = (
    df_session_base["num_hotel_trips"] / df_session_base["num_trips"]
)

# Flight Fanatic Index
df_session_base["flight_fanatic_index"] = (
  df_session_base["num_flight_trips"] / df_session_base["num_trips"]
)

# Bundle Lover Index
df_session_base["bundle_index"] = (
    df_session_base["num_bundle_trips"] / df_session_base["num_trips"]
)

# Replace infinities and NaNs (caused by division by zero)
df_session_base.replace([np.inf, -np.inf], 0, inplace=True)
df_session_base.fillna(0, inplace=True)

# Preview
df_session_base[[
    "user_id",
    "num_trips",
    "num_hotel_trips",
    "num_flight_trips",
    "num_bundle_trips",
    "hotel_hunter_index",
    "flight_fanatic_index",
    "bundle_index"
]].head()

"""# Session-level data was aggregated to user level, and multiple feature tables were merged into a unified user feature dataset containing engagement, trip behavior, cancellations, and preference indices."""

# 1) Base session features
df_user_base = (
    df_session_base_for_feature
    .groupby("user_id")
    .agg(
        num_sessions=("session_id","count"),
        num_empty_sessions=("empty_session","sum"),
        num_clicks=("page_clicks","sum"),
        avg_clicks_per_session=("page_clicks","mean"),
        avg_session_duration=("session_duration","mean"),
        std_session_duration=("session_duration","std")
    )
    .reset_index()
)

df_user_base["std_session_duration"] = df_user_base["std_session_duration"].fillna(0)

# 2) Merge all other user-level feature tables at once
dfs_to_merge = [df_user_base1, df_user_base2, df_user_base3]

for d in dfs_to_merge:
    df_user_base = df_user_base.merge(d, on="user_id", how="left")

# 3) Add indices
indices = (
    df_session_base
    [["user_id","hotel_hunter_index","flight_fanatic_index","bundle_index"]]
    .drop_duplicates("user_id")
)

df_user_base = df_user_base.merge(indices, on="user_id", how="left")

# save dataframe
df_user_base.to_csv(f'{directory}/user_base.csv', index=False)
df_user_base.describe().T

"""# Build Final User Base Feature Data Frame

session features <- canceled features <- trip features <- static features
"""

# a master user dataset created also I validated its structure and completeness
df_user_base_final = pd.merge(df_user_base,df_user_base1, on='user_id', how='left')
df_user_base_final = pd.merge(df_user_base_final, df_user_base2, on='user_id', how='left')
df_user_base_final = pd.merge(df_user_base_final, df_user_base3, on='user_id', how='left')

print(df_user_base_final.shape)
print(df_user_base_final.columns)
print(df_user_base_final.isna().sum())
df_user_base_final.head()

# save dataframe
df_user_base.to_csv(f'{directory}/user_base.csv', index=False)
df_user_base.describe().T
##print(df_user_base_final.shape) ## 31 columns not 32

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = f'{directory}/user_base_final.csv'

# Download the file

# =====================
# user_pca_analysis.py
# =====================
# -*- coding: utf-8 -*-
"""user_pca_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOiCTuZaS6do-PHnOjAS6vFgY6CRp7uP

#PCA Analysis
Principal Component Analysis (PCA) is a method for dimensionality reduction. Its goal is to reduce the complexity of datasets with many variables (dimensions) by summarizing the most important information into a smaller number of new, uncorrelated variables, known as principal components. This helps to identify and visualize patterns in the data.
"""

# import necessary libraries
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Connect to google drive-Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# read the data file from related directory
directory = "/content/drive/MyDrive/Travel tide project"

df_users = pd.read_csv(f'{directory}/user_base.csv', index_col="user_id")
print(df_users.columns)

"""# To prepare our data for clustering, I encoded demographic categorical and boolean columns into numeric values (1 and 0) so they can be used in machine learning models."""

# To prepare our data for clustering, I encoded demographic categorical and boolean columns into numeric values (1 and 0) so
# they can be used in machine learning models.
df_users["gender"] = df_users["gender"].map({"F": 0, "M": 1, "O":2})
df_users["married"] = df_users["married"].astype(int)
df_users["has_children"] = df_users["has_children"].astype(int)

print('different countries', df_users["home_country"].nunique())

#I've decided to keep Home Country

df_users["home_country"] = (df_users["home_country"] == 'usa').astype("int")

df_users.head()

# check
df_users.dtypes

# PCA cannot work with NaN values therefore I Replaced NaN with 0 where possible

print(df_users.isnull().sum())

#drop gender NaN
#df_users["gender"].dropna(inplace=True)

df_users.fillna(0, inplace=True)

# Before apply PCA, check if there is null in database
print(df_users.isnull().sum())

"""# Data scaling(normalizing numerical features)"""

# Data scaling(normalizing numerical features)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_users)
customers_scaled = pd.DataFrame(X_scaled)
customers_scaled.index = df_users.index
customers_scaled.columns = df_users.columns

# check/print scaled data columns
customers_scaled.head(2)

"""# I want to preserve 95% of the data variance, so I have to have enough pca components. The number of components we need for this should be determined by Python."""

# We want to be able to preserve 95% of the data variance, so we have to have enough pca components

#The number of components we need for this should be determined by Python.

var_exp = 0.95
pca = PCA(n_components = var_exp, random_state= 42)
customers_pca = pca.fit_transform(customers_scaled)


customers_pca = pd.DataFrame(customers_pca)
customers_pca.index = customers_scaled.index
customers_pca.columns = [f"pca_{i}"for i in range(customers_pca.shape[1])]

customers_pca.to_csv(f'{directory}/user_pca.csv')
print(customers_pca.shape)
customers_pca.head()

"""# # Display eigenvalues ​​- coefficients that are crucial for every PC"""

# # Display eigenvalues ​​- coefficients that are crucial for every PC
component_matrix = pd.DataFrame(pca.components_).T
component_matrix.columns = [f"pca_{i}"for i in range(component_matrix.shape[1])]
component_matrix.index = customers_scaled.columns
component_matrix

"""# in this step we perform PCA. n_commponents = None in order to calculate the variance of all possible principal components."""

# in this step we perform PCA. n_commponents = None in order to
# calculate the variance of all possible principal components.
pca = PCA(n_components=None, random_state= 42)
pca.fit(X_scaled)

# Then Calculate Cumulative Variance
# The 'explained_variance_ratio_' indicates the percentage of the variance that is explained by each principal component.

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# display graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')
plt.title('Cumulative variance explained by the principal components')
plt.xlabel('Number of principle components')
plt.ylabel('Cumulative explained variance (%)')
plt.grid(True)
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variace thresholds ')

# we have to find the point at which the cumulative variance exceeds 95%
n_components_95 = np.where(cumulative_variance >= var_exp)[0][0] + 1
plt.axvline(x=n_components_95, color='g', linestyle='--', label=f'{n_components_95} Komponenten für {var_exp * 100}%')
plt.text(n_components_95, 0.5, f'{n_components_95} Components', color='g', ha='right', va='center')

plt.legend(loc='lower right')
plt.tight_layout()
plt.show()

cumulative_variance

"""Now let's look at which combination of variables contributes most to our component value. This will help us understand our components a little better.

This PCA heatmap shows that most principal components are driven by a small number of strongly loading features, indicating that the model has learned several distinct and interpretable behavioral dimensions rather than diffuse combinations of all variables. In particular, clusters of high loadings appear around logically related groups such as flight activity and spending, hotel and accommodation behavior, online engagement (sessions, clicks, duration), seasonality of travel, and group or family travel characteristics. This suggests that the underlying structure of the data is organized around clear travel‐behavior patterns rather than being dominated by demographic attributes.

The main takeaway is that behavioral features contribute far more to explaining variance than demographic indicators, which appear only in a limited number of components and with lower overall influence. Practically, this means the reduced PCA space captures meaningful traveler archetypes—such as frequent flyers, hotel-focused travelers, deal-seekers, or high-engagement users—which can be leveraged for segmentation, clustering, or downstream modeling. Overall, the heatmap supports that PCA has successfully compressed correlated variables into a smaller set of interpretable latent factors while preserving the most important information in the dataset.

The heatmap confirms that TravelTide users naturally separate into meaningful behavioral groups based on engagement, spending, trip composition, discount sensitivity, and life stage—making PCA + clustering an appropriate approach.
"""

from google.colab import files

# Save the DataFrame to the current working directory
customers_pca.to_csv("user_pca_analysis.csv", index=False)

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = 'user_pca_analysis.csv'

# Download the file
files.download(file_path)

# =====================
# customer_segmentation_kmeans.py
# =====================
# -*- coding: utf-8 -*-
"""customer_segmentation_kmeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXRsaSjgul48eimNNBrCR14uGY5OHsO8
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Import needed libraries
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# connect directoy and data file
directory = "/content/drive/MyDrive/Travel tide project"

user_pca = pd.read_csv(f'{directory}/user_pca.csv', index_col=0)
user_pca.head()

user_pca.shape

print(user_pca.columns)

"""# Define the range for the number of clusters to be tested (e.g., 2 to 10)"""

# Define the range for the number of clusters to be tested (e.g., 2 to 10)
range_n_clusters = range(3, 11)
silhouette_scores = []
inertia_values = []

# Perform K-means for each k-value and calculate the silhouette score.
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(user_pca)
    silhouette_avg = silhouette_score(user_pca, cluster_labels)
    silhouette_scores.append(silhouette_avg)
    inertia_values.append(kmeans.inertia_)

# Find best silhouette k
optimal_n_clusters = range_n_clusters[np.argmax(silhouette_scores)]

# Create subplots
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14, 5))

#  Create Elbow Plot
ax1.plot(range_n_clusters, inertia_values, marker='o')
ax1.set_title("Elbow Method")
ax1.set_xlabel("Number of Clusters (k)")
ax1.set_ylabel("Inertia")
ax1.grid(True)

# --- Silhouette Plot ---
ax2.plot(range_n_clusters, silhouette_scores, marker='o')
ax2.axvline(optimal_n_clusters, color='red', linestyle='--', label=f'Optimal k = {optimal_n_clusters}')
ax2.set_title("Silhouette Analysis")
ax2.set_xlabel("Number of Clusters (k)")
ax2.set_ylabel("Silhouette Score")
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

print(f"Optimal number of clusters based on Silhouette: {optimal_n_clusters}")

#Usually we choose value of K (number of clusters) then we evaluate this number of with Silhouette
#Analysis  and the K that gives the highes silhouette score issnually considere the best k but in our
#case however number cluster for silhouette analysis is 3, because we have five perks and also
#with Elbow method suggest 5-6 clusters so optional n =5
optimal_n_clusters = 5

"""# K-mean clustering with five clusters"""

from matplotlib.colors import ListedColormap

# Custom colors for clusters
custom_colors = [
    "#FDE68A",  # light yellow (group 0)
    "#A7F3D0",  # light green  (group 1)
    "#60A5FA",  # blue
    "#F87171",  # red
    "#A78BFA"   # purple
]

cmap = ListedColormap(custom_colors)

# Perform K-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)
user_pca['group'] = kmeans.fit_predict(user_pca)

plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    user_pca.iloc[:, 0],
    user_pca.iloc[:, 1],
    c=user_pca['group'],
    cmap=cmap,
    s=50,
    alpha=0.7
)

plt.title(f'K-Means Clustering with {optimal_n_clusters} Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.grid(True)

# Add legend
legend = plt.legend(*scatter.legend_elements(), title="Groups")
plt.show()

cluster_profiles = user_pca.groupby('group').mean()
display(cluster_profiles.round(2))

"""This K-means clustering plot shows that most of the separation between the five clusters occurs along PCA Component 1, indicating that the primary dimension driving segmentation is captured on the horizontal axis. One cluster (red) is clearly separated on the right with high PC1 values, suggesting a distinct group with strong underlying behavioral characteristics relative to others. On the left side, another cluster (green) occupies low PC1 values, forming a contrasting group. The remaining clusters (blue, purple, yellow) are more concentrated around the center and partially overlap, implying they share some similarities but still differ in subtler ways captured by the combination of PCA Component 1 and 2.

The key takeaway is that the data contains one or two strongly distinct traveler segments, plus several moderately differentiated groups clustered around average behavior. PCA Component 2 mainly adds vertical spread but does not create strong horizontal separations, meaning it refines distinctions rather than defining primary segments. Overall, the clustering appears reasonable: there is meaningful structure, but not extreme separation, which is typical for real behavioral data. These clusters can now be interpreted by examining feature averages per cluster to assign practical labels such as high-activity travelers, low-engagement users, deal-seekers, or group-oriented travelers
"""

#save the user_id -> group connection in a csv

user_pca['group'].to_csv(f'{directory}/user_segment.csv')
user_pca['group']

# count of users per cluster
clusters = KMeans(n_clusters=optimal_n_clusters, random_state=42)\
            .fit_predict(user_pca.select_dtypes(include=["int64","float64"]))

output_df = user_pca.assign(cluster=clusters).reset_index()
output_df.to_csv("user_base_with_clusters.csv", index=False)

print("Clustering completed.")
print(output_df.cluster.value_counts())

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = f'{directory}/user_segment.csv'

# Download the file
files.download(file_path)

# =====================
# cluster_profiling.py
# =====================
# -*- coding: utf-8 -*-
"""cluster_profiling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13TE3IGtqo6zR20JWOL6ALBlaOPfyJogV
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

"""#Cluster profiling based on pca and kmeans analysis"""

import numpy as np
import pandas as pd

directory = "/content/drive/MyDrive/Travel tide project"

# List files in the directory to verify filename
!ls "{directory}"

df_user = pd.read_csv(f'{directory}/user_base.csv')
df_segment = pd.read_csv(f'{directory}/user_segment.csv') # Corrected filename

df = pd.merge(df_user, df_segment, on='user_id')
print(df.shape)
print(df.dtypes)

df_user.groupby('gender').size()

#  we remove features that we dont analyze
columns=["user_id"]
# Check if 'user_id' column exists before dropping
if "user_id" in df.columns:
    df.drop(columns=columns, inplace=True)

# change booleans to int  -> good to build the heatmaps

# convert binary categoricals
df["gender"] = df["gender"].map({"F": 0, "M": 1, "O":2})
df["married"] = df["married"].astype(int)
df["has_children"] = df["has_children"].astype(int)

df["home_country"] = (df["home_country"] == 'canada').astype(int)

df.head().round(2)

# group= cluster
df.groupby('group').size()

# Create a temporary DataFrame with original gender values for crosstabulation
temp_df = pd.merge(df_user, df_segment, on='user_id')

print("Crosstabulation using original 'gender' values:")
original_gender_crosstab = pd.crosstab(temp_df["group"], temp_df["gender"])
display(original_gender_crosstab)

# Create a temporary DataFrame with original gender and having children values for crosstabulation
pd.crosstab(temp_df["group"],[temp_df["gender"],temp_df["has_children"]])

# Median values of all numeric features per group
group_summary = df.groupby("group").median(numeric_only=True).T.round(2)
print(df.groupby("group").size().reset_index())
group_summary

# Mean values of all numeric features per group
group_summary_mean = df.groupby("group").mean(numeric_only=True).T.round(2)
group_summary_mean

#box plots
import matplotlib.pyplot as plt
import seaborn as sns

# Example: distribution of money spent by group
sns.boxplot(x="group", y="avg_money_spent_per_hotel", data=df)
plt.show()

# Example: average km flown by group
sns.boxplot(x="group", y="avg_flight_km", data=df)
plt.show()

# box plots canceled trips and empty sessions per groups
import matplotlib.pyplot as plt
import seaborn as sns

selected_features = [
    "num_canceled_trips",
    "num_empty_sessions",
]

for feature in selected_features:
    plt.figure(figsize=(10, 5))
    sns.boxenplot(x="group", y=feature, data=df)
    plt.title(f"Boxenplot of {feature} by Cluster")
    plt.show()

# Heatmap of group means
import matplotlib.pyplot as plt
import seaborn as sns
group_summary = df.groupby("group").median(numeric_only=True).T.round(2)
plt.figure(figsize=(12, 6))
sns.heatmap(group_summary, cmap="coolwarm", annot=True, fmt=".1f")
plt.title("Feature Medians per Group")
plt.show()

#df.groupby("group").mean().T

# Heatmap of group means
group_summary_mean = df.groupby("group").mean(numeric_only=True).T
plt.figure(figsize=(12, 6))
sns.heatmap(group_summary_mean, cmap="coolwarm", annot=True, fmt=".1f")
plt.title("Feature Means per Group")
plt.show()

"""These two heatmaps together describe the **behavioral profiles of each cluster** and confirm that the clusters differ mainly in *engagement level, travel intensity, and spending power*. **Group 3** clearly stands out as the **most active and valuable segment**: it has the highest number of sessions, clicks, flights, and rooms, along with the highest average spend per flight and per hotel. **Group 4** shows similarly high engagement and strong spending, but slightly below Group 3, suggesting another high-value segment with somewhat less intensity. In contrast, **Group 1 consistently has the lowest values** across sessions, clicks, flights, rooms, and spending, identifying it as a low-engagement, low-activity group. Groups 0 and 2 fall between these extremes, with moderate engagement and travel frequency.

A second important pattern is that **spending per kilometer and hotel costs vary substantially across clusters**, with Group 0 showing unusually high average/median money per km, indicating a niche segment that tends to purchase more expensive trips despite only moderate activity. The similarity between mean and median patterns suggests the clusters are relatively stable and not driven by extreme outliers, although higher means than medians for some monetary features imply a few heavy spenders within certain groups. Overall, the clusters can be summarized as: Group 1 = minimal users, Group 0 = moderate users with premium trip characteristics, Group 2 = average travelers, Group 4 = high-engagement travelers, and Group 3 = power users / frequent high spenders. These profiles provide a strong foundation for targeted marketing, personalization, and differentiated product offerings.

#### Perk Assignments to Clusters

| Cluster | Data-Driven Summary                                                                                                                                                                                                                                                                                                                                   | Assigned Perk              | Justification                                                                                                                                                                                                                                                                                                                                                                                               |
| :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **0**   | Younger, hotel-focused, high hotel spenders with low overall travel engagement, taking fewer but more luxurious or longer hotel-centric trips. Low flight activity.                                                                                                                                                                               | **Free hotel night with flight** | While they are hotel-focused, a "free hotel night with flight" could incentivize them to combine their hotel stays with flights, potentially increasing their overall engagement and flight activity. This is a compromise from the ideal "Premium Hotel Upgrades" due to the limited options provided.                                                  |
| **1**   | Highly engaged, frequent travelers who actively seek and use discounts, often bundling their travel. Likely to be older and travel year-round, with high session counts, clicks, and usage of flight/hotel discounts.                                                                                                                               | **Exclusive discount**     | This cluster already shows high engagement and actively seeks discounts. Exclusive discounts would directly reward their behavior and encourage continued high activity and loyalty. This aligns perfectly with their discount-seeking nature.                                                                                                 |
| **2**   | Efficient and active travelers, similar to Cluster 1 but with less emphasis on discounts and lower hotel spending. Value bundled services, with moderate engagement, flights, and hotels.                                                                                                                                                          | **Free meal**              | This cluster is efficient and active. A free meal can add perceived value and convenience to their trips without directly focusing on discounts or large financial incentives. It's a simple, tangible benefit that enhances their travel experience, especially if they are looking for efficiency.                                                     |
| **3**   | High-spending, often family or group travelers, taking long-haul flights, booking significantly in advance, and less interested in hotels as a primary focus. Highest `avg_money_spent_per_flight`, `avg_flight_km`, `avg_bags`, `avg_rooms`, `avg_seats`, `num_fam_and_fri_trips`, and `avg_time_after_booking`.                                                  | **Free checked bag**       | This cluster takes long-haul flights and often travels with family/friends (`num_fam_and_fri_trips`, `avg_bags`, `avg_seats` are high). A free checked bag directly addresses a practical need and cost associated with their travel style, providing significant value for group or long-distance trips.                                               |
| **4**   | Highly engaged users who spend a lot of time browsing and exploring options, possibly looking for shorter stays, and often interested in bundled deals. Highest `avg_clicks_per_session` and `avg_session_duration`. High `bundle_index`, `hotel_hunter_index`, and `flight_fanatic_index`, but lower `avg_night`. | **Free cancellation fee**  | This cluster is highly engaged in browsing and exploration, suggesting they might be indecisive or frequently changing plans. A "free cancellation fee" perk offers flexibility and reduces booking friction, encouraging them to book more frequently knowing they have an option to change without penalty, fitting their exploratory nature. |
"""



if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""trip_and_user_level_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWk7Bywq-FWrtie1AeU6VmUEwjzwMZzd
"""

# import libraries
import numpy as np
import pandas as pd

#connect to google drive
from google.colab import drive
drive.mount('/content/drive')

# connect to files/read files, shape and columns
directory = "/content/drive/MyDrive/Travel tide project"

df_session_base = pd.read_csv(directory + '//travel_tide.csv')
df_nc_sessions = pd.read_csv(directory + '/not_canceled_trips.csv')

print(df_session_base.shape)
print(df_nc_sessions.shape)

print(df_nc_sessions.columns)
df_session_base.head(2)

df_trips = df_nc_sessions.copy()

# I converted several timestamp columns into proper datetime format across three datasets so that time-based
# calculations and features can be created reliably.
cols = ['session_start', 'session_end', 'birthdate', 'sign_up_date', 'departure_time', 'return_time', 'check_in_time', 'check_out_time']

for c in cols:
  df_trips[c] = pd.to_datetime(df_trips[c], format='mixed')
  # df_session_base does not contain these timestamp columns, so this line is removed.
  # df_session_base[c] = pd.to_datetime(df_session_base[c], format='mixed')
  df_nc_sessions[c] = pd.to_datetime(df_nc_sessions[c], format='mixed')

"""# Feature Engineering for Session Base"""

# I created a flag for sessions without trips and then summarized session behavior per user
# to build a user-level engagement feature table.


# The original df_session_base (from session_base.csv) does not contain
# 'trip_id', 'session_start', 'session_end', or 'page_clicks' needed for these operations.
# df_trips (copy of df_nc_sessions) contains all these columns.
# For the purpose of this feature engineering block, I used will df_trips as the source.


df_session_base_for_feature = df_trips.copy()


#identify empty sessions
df_session_base_for_feature['empty_session'] = df_session_base_for_feature['trip_id'].isna().astype(int)

# Calculate session duration
df_session_base_for_feature['session_duration'] = (df_session_base_for_feature['session_end'] - df_session_base_for_feature['session_start']).dt.total_seconds() / 60

# aggregate session information per user
df_user_base = df_session_base_for_feature.groupby('user_id').agg(
 num_sessions = ('session_id', 'count'),
 avg_clicks_per_session = ('page_clicks', 'mean'),
 num_empty_sessions = ('empty_session', 'sum'),
 avg_session_duration = ('session_duration', 'mean'),
 std_session_duration = ('session_duration', 'std'),
  num_clicks= ('page_clicks', 'sum'),
    avg_session_clicks= ('page_clicks', 'mean')
).reset_index()

df_user_base.head(2).round(2)

# check
print(df_user_base.shape)
df_user_base.isnull().sum()

"""# Compare number of Trips with Canceled Trips"""

# counts rows per cancellation status
df_session_base.groupby("cancellation").size().reset_index(name="count")

"""# Breakdown of Cancelled Trips by Booking Type"""

# Breakdown of Cancelled Trips by Booking Type
cancelled_trips_df = df_session_base[df_session_base['cancellation'] == True]

# Get unique cancelled trip IDs
unique_cancelled_trip_ids = cancelled_trips_df['trip_id'].unique()

# Define trip_summary using df_session_base to cover all bookings
trip_summary = df_session_base.groupby("trip_id").agg(
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

# Filter trip_summary to get details for only cancelled trips
cancelled_trip_details = trip_summary[trip_summary['trip_id'].isin(unique_cancelled_trip_ids)]


# Count the occurrences of each combination
cancellation_type_counts = cancelled_trip_details[['has_flight', 'has_hotel']].value_counts().reset_index(name='count')
cancellation_type_counts['booking_type'] = cancellation_type_counts.apply(
    lambda row: 'Flight and Hotel' if row['has_flight'] and row['has_hotel']
    else ('Flight Only' if row['has_flight']
    else ('Hotel Only' if row['has_hotel']
    else 'Unknown/Other')),
    axis=1
)

print("Breakdown of Cancelled Trips by Booking Type:")
display(cancellation_type_counts[['booking_type', 'count']])

"""#Only non-canceled trips and printed how many rows (sessions) those trips have"""

#Only non-canceled trips and printed how many rows (sessions) those trips have.
df_temp = df_session_base.dropna(subset = ['trip_id'])

# Define canceled_trip_ids from df_session_base where cancellation is True
canceled_trip_ids = df_session_base[df_session_base['cancellation'] == True]['trip_id'].unique()

df_not_canceled_trips = df_temp[~df_temp['trip_id'].isin(canceled_trip_ids)]
print(df_not_canceled_trips.shape[0])

# Re-create df_user_base1 and df_user_base2 to ensure a clean state before merging

canceled_df = df_session_base[(df_session_base['cancellation'] == True) & (df_session_base['trip_id'].notna())]
df_user_base1 = canceled_df.groupby('user_id').agg(
    num_canceled_trips = ('trip_id', 'nunique')
).reset_index()
df_user_base2 = df_nc_sessions.groupby('user_id').agg(
    num_not_canceled_trips = ('trip_id', 'nunique')
).reset_index()

df_user_base1 = pd.merge(df_user_base1, df_user_base2, on='user_id', how='outer')
print(df_user_base1.shape)
df_user_base1.head()

##df_user_base1[(df_user_base1['num_canceled_trips'] > 0 ) & (df_user_base1['num_not_canceled_trips'] > 0)]

"""# Features and columns engineering"""

#check if df_trips is complete
assert df_nc_sessions.shape[0] == 15489

# I created new columns group_trip, pair_trip,business_week_trip, weekend_trip,discount_trip,season
def group_trip(row):
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and row['seats'] > 2 and row['rooms'] > 1:
    return 1
  #Bus/Bahn/Auto Reise
  elif row['flight_booked'] == False and row['hotel_booked'] == True and row['rooms'] > 1 :
    return 1
  else:
    return 0

def pair_trip(row):
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and not pd.isna(row['seats']) and not pd.isna(row['rooms']) and row['seats'] == 2 and row['rooms']==1:
    return 1
  else:
    return 0

def business_week_trip(row):
  birth_date = pd.to_datetime(row['birthdate'], format='mixed')
  age = (pd.Timestamp.today() - birth_date).days / 365
  departure = row['departure_time']
  return_ = row['return_time']
  #check for flights during the week
  ## Weekday: Monday = 0, Sunday = 6
  # Business days are 0 to 4 (Monday to Friday)
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and row['seats'] == 1 and row['nights'] >= 1 and row['nights'] < 6 and age >= 25 and age <= 60 and (departure.weekday() <= 4) and (return_.weekday() <= 4):
    return 1
  else:
    return 0

def weekend_trip(row):
  departure = row['departure_time']
  return_ = row['return_time']
  # Friday = 4, Sunday = 6
  if row['flight_booked'] == True and row['return_flight_booked'] == True and row['hotel_booked'] == True and not pd.isna(row['nights']) and row['nights'] <= 2 and (departure.weekday() >= 4) and (return_.weekday() <= 6):
    return 1
  else:
    return 0

def season_trip(row):
  if row['departure_time'].month in [12, 1, 2]:
    return "winter"
  if row['departure_time'].month in [6, 7, 8]:
    return "summer"
  if row['departure_time'].month in [9, 10, 11]:
    return "fall"
  else:
    return "spring"

def discount_trip(row):
  if row['flight_discount'] == True or row['hotel_discount'] == True:
    return 1
  else:
    return 0

df_nc_sessions['group_trip'] = df_nc_sessions.apply(group_trip, axis = 1)
df_nc_sessions['pair_trip'] = df_nc_sessions.apply(pair_trip, axis = 1)
df_nc_sessions['business_week_trip'] = df_nc_sessions.apply(business_week_trip, axis = 1)
df_nc_sessions['weekend_trip'] = df_nc_sessions.apply(weekend_trip, axis = 1)
df_nc_sessions['discount_trip'] = df_nc_sessions.apply(discount_trip, axis = 1)
df_nc_sessions['season'] = df_nc_sessions.apply(season_trip, axis = 1)

# I created multiple new features measuring flight count, flight and hotel spending, and booking lead time to better
#understand trip purchasing behavior
df_trips['num_flights'] = np.where(
    #if
    (df_trips['flight_booked'] == True) & (df_trips['return_flight_booked'] == True),
    2,
    #else
    np.where(
        (df_trips['flight_booked'] == True) & (df_trips['return_flight_booked'] == False),
        1,0
    )
)

df_trips['money_spent_per_flight'] = np.where(
    (df_trips['flight_discount'] == True),
    df_trips['base_fare_usd'] * (1 - df_trips['flight_discount_amount']),
    df_trips['base_fare_usd']
)

df_trips['money_spent_per_seat'] = df_trips['money_spent_per_flight'] / df_trips['seats']

# money spent per hotel (total)
# rooms * nights * price_per_room_per_night
df_trips['money_spent_total_hotel'] = df_trips['rooms'] * df_trips['nights'] * df_trips['hotel_price_per_room_night_usd']

df_trips['money_spent_per_hotel'] = np.where(
    (df_trips['hotel_discount'] == True),
    df_trips['money_spent_total_hotel'] * (1 - df_trips['hotel_discount_amount']),
    df_trips['money_spent_total_hotel']
)

# Ensure relevant columns are datetime objects
df_trips['departure_time'] = pd.to_datetime(df_trips['departure_time'], errors='coerce')
df_trips['session_end'] = pd.to_datetime(df_trips['session_end'], errors='coerce')
df_trips['check_in_time'] = pd.to_datetime(df_trips['check_in_time'], errors='coerce')

# Time after booking
df_trips['time_after_booking'] = np.where(
    (df_trips['flight_booked'] == True),
    (df_trips['departure_time'] - df_trips['session_end']).dt.days,
    (df_trips['check_in_time'] - df_trips['session_end']).dt.days
)
df_trips[['user_id', 'money_spent_total_hotel', 'departure_time', 'time_after_booking']].head()

# I engineered a binary feature identifying group travelers (family and friends) and a categorical feature labeling the season in which each trip occurred.
df_trips['family_and_friends'] = np.where(
    ((df_trips['flight_booked'] == True) & (df_trips['seats'] > 1)),
    1,0
)

# Give back the name of the season in which the customer traveled.
#return 'winter' if trip happened in winter
def season_helper(row):
  #define time
  time = row['departure_time'] if row['flight_booked'] == True else row['check_in_time']

  # Ensure time is a datetime object or NaT and handle NaT values
  if pd.isna(time):
      return None # Or 'unknown', or a placeholder for missing dates

  month = time.month
  if month in [12, 1, 2]:
    return 'winter'
  elif month in [3, 4, 5]:
    return 'spring'
  elif month in [6, 7, 8]:
    return 'summer'
  else:
    return 'fall'


df_trips['season'] = df_trips.apply(season_helper, axis=1)

df_trips[['user_id', 'season', 'family_and_friends', 'trip_id']].head()

#Check if df_trips is complete
assert df_trips.shape[0] == 15489

"""# categorical season column converted into multiple binary numeric columns using one-hot encoding and appended them to your dataset"""

# categorical season column converted into multiple binary numeric columns using one-hot encoding and appended them to your dataset
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False)
season_encoded = encoder.fit_transform(df_trips[['season']])
df_season_encoded = pd.DataFrame(season_encoded, columns=encoder.get_feature_names_out(['season']))
df_trips = pd.concat([df_trips, df_season_encoded], axis=1)

#to confirm that df contains exactly 15,489 rows, ensuring data integrity.
assert df_trips.shape[0] == 15489

"""# Fly distance calculation Haversine distance"""

# Fly distance calculation Haversine distance
import math
def haversine_distance(lat1, lon1, lat2, lon2):
  if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):
    return np.nan
  # Convert latitude and longitude from degrees to radians
  R = 6371
  lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
  dlon = lon2 - lon1
  dlat = lat2 - lat1

  a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2
  c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
  distance = R * c
  return distance

# I created a new feature that calculates the kilometer distance between home
#and destination airports for each trip using geographic coordinates.
df_trips['distance_km'] = df_trips.apply(lambda row: haversine_distance(row['home_airport_lat'], row['home_airport_lon'],
                                                                        row['destination_airport_lat'], row['destination_airport_lon']), axis= 1)

#calculated money spent per flight KM
df_trips['money_spent_per_km'] = df_trips['money_spent_per_flight'] / df_trips['distance_km']

df_trips.columns

"""# Before aggregation, ensure unique columns to avoid AttributeError Keep the first occurrence of duplicate columns"""

# Before aggregation, ensure unique columns to avoid AttributeError
# Keep the first occurrence of duplicate columns
df_trips_cleaned = df_trips.loc[:, ~df_trips.columns.duplicated()]

# Features
df_user_base2 = df_trips_cleaned.groupby('user_id').agg(
 num_flights = ('num_flights', 'sum'),
 num_hotels = ('hotel_booked', 'sum'),
 avg_bags = ('checked_bags', 'mean'),
 num_flight_discounts = ('flight_discount', 'sum'),
 num_hotel_discounts = ('hotel_discount', 'sum'),
 avg_night = ('nights', 'mean'),
 avg_rooms = ('rooms', 'mean'),
 avg_seats = ('seats', 'mean'),
 avg_money_spent_per_flight = ('money_spent_per_flight', 'mean'),
 avg_money_spent_per_seat = ('money_spent_per_seat', 'mean'),
 avg_money_spent_per_hotel = ('money_spent_per_hotel', 'mean'),
 avg_time_after_booking = ('time_after_booking', 'mean'),
 num_fam_and_fri_trips = ('family_and_friends', 'sum'),
 num_winter = ('season_winter', 'sum'),
 num_spring = ('season_spring', 'sum'),
 num_summer = ('season_summer', 'sum'),
 num_fall = ('season_fall', 'sum'),
 avg_flight_km = ('distance_km', 'mean'),
 avg_money_spent_per_km = ('money_spent_per_km', 'mean')
).reset_index()
df_user_base2.head().round(2)

# to check number of nulls in df
print(df_user_base2.shape)
df_user_base2.isna().sum()

#missing-value cleanup
df_user_base2.fillna(0, inplace=True)

"""# Static User Features"""

#Importing database from SQL
from sqlalchemy import create_engine
import sqlalchemy as sa

# Define your PostgreSQL connection parameters
username = 'USERNAME'
password = 'PASSWORD'
host = 'ep-noisy-flower-846766.us-east-2.aws.neon.tech'
port = '5432'             # default PostgreSQL port
database = 'TravelTide'

# Create connection string using SQLAlchemy
conn_str = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'
engine = create_engine(conn_str)

users = pd.read_sql("SELECT * FROM users", engine)
users.head(2)

# converted birthdates to datetime, computed user ages, and created a demographic user table with
# key personal attributes.
users['birthdate'] = pd.to_datetime(users['birthdate'], format='mixed')
users['age'] = (pd.to_datetime('today') - users['birthdate']).dt.days // 365

df_user_base3 = users[['user_id', 'gender', 'married', 'has_children', 'home_country', 'age']].copy()
df_user_base3.head(3)

"""Three indices calculated:
1- Flight Hunter index
2- Hotel Hunter Index
3-Bundle lover index
"""

import pandas as pd
import numpy as np


# Ensure trip_id is clean for processing as it's used in user_summary and trip_summary creation
df_session_base["trip_id"] = df_session_base["trip_id"].astype(str).str.strip().replace(["", "nan", "None", "null"], pd.NA)

# Re-create user_summary to get 'num_trips' and other user-level metrics
sessions_per_user = df_session_base.groupby("user_id")["session_id"].nunique().reset_index(name="num_sessions")
trips_per_user = (
    df_session_base[df_session_base["trip_id"].notna()]
    .groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_trips")
)
canceled_df = df_session_base[(df_session_base['cancellation'] == True) & (df_session_base['trip_id'].notna())]
canceled_trips_per_user = (
    canceled_df.groupby("user_id")["trip_id"]
    .nunique()
    .reset_index(name="num_canceled_trips")
)
user_summary = (
    sessions_per_user
    .merge(trips_per_user, on="user_id", how="left")
    .merge(canceled_trips_per_user, on="user_id", how="left")
)
user_summary["num_trips"] = user_summary["num_trips"].fillna(0).astype(int)
user_summary["num_canceled_trips"] = user_summary["num_canceled_trips"].fillna(0).astype(int)


# Re-create df_trip for `trip_summary` (from previous cell Xz7rumqlbk0q's logic)
df_trip = df_session_base[df_session_base["trip_id"].notna()].copy()
num_cols_df_trip = ["seats", "base_fare_usd", "nights", "rooms", "hotel_price_per_room_night_usd"]
for c in num_cols_df_trip:
    df_trip[c] = pd.to_numeric(df_trip[c], errors="coerce").fillna(0)
df_trip.loc[df_trip["nights"] <= 0, "nights"] = 0

trip_summary = df_trip.groupby("trip_id").agg(
    user_id=("user_id", "first"), # Get user_id for merging later
    has_flight=("departure_time", lambda x: x.notna().any()),
    has_hotel=("hotel_name", lambda x: x.notna().any())
).reset_index()

# Create an 'is_bundle' column in trip_summary first
trip_summary['is_bundle'] = trip_summary['has_flight'] & trip_summary['has_hotel']

# Calculate user-level trip type counts from trip_summary
user_trip_types = trip_summary.groupby('user_id').agg(
    num_flight_trips=('has_flight', lambda x: (x == True).sum()),
    num_hotel_trips=('has_hotel', lambda x: (x == True).sum()),
    num_bundle_trips=('is_bundle', lambda x: (x == True).sum()) # Use the new 'is_bundle' column
).reset_index()


# Merge user-level trip counts into user_summary
user_summary = user_summary.merge(user_trip_types, on='user_id', how='left')

# Fill NaNs that might result if a user has no flights/hotels/bundles
user_summary[["num_flight_trips", "num_hotel_trips", "num_bundle_trips"]] = \
    user_summary[["num_flight_trips", "num_hotel_trips", "num_bundle_trips"]].fillna(0).astype(int)


#  Merge these user-level aggregates into df_session_base
# Prepare the columns before merging to avoid KeyError later
cols_to_merge_from_user_summary = ['num_sessions', 'num_trips', 'num_canceled_trips',
                                   'num_flight_trips', 'num_hotel_trips', 'num_bundle_trips']

user_summary_to_merge = user_summary[['user_id'] + cols_to_merge_from_user_summary].copy()
# Fill NaNs and convert type in the temporary DataFrame before merge
for col in cols_to_merge_from_user_summary:
    user_summary_to_merge[col] = user_summary_to_merge[col].fillna(0).astype(int)

df_session_base = df_session_base.merge(user_summary_to_merge, on='user_id', how='left')

# The fillna and astype for these specific columns on df_session_base is now done via the user_summary_to_merge


# ensure coluumns are numeric
cols = ["num_trips", "num_hotel_trips", "num_flight_trips", "num_bundle_trips"]
for c in cols:
    df_session_base[c] = pd.to_numeric(df_session_base[c], errors="coerce").fillna(0)

# Hotel Hunter Index
df_session_base["hotel_hunter_index"] = (
    df_session_base["num_hotel_trips"] / df_session_base["num_trips"]
)

# Flight Fanatic Index
df_session_base["flight_fanatic_index"] = (
  df_session_base["num_flight_trips"] / df_session_base["num_trips"]
)

# Bundle Lover Index
df_session_base["bundle_index"] = (
    df_session_base["num_bundle_trips"] / df_session_base["num_trips"]
)

# Replace infinities and NaNs (caused by division by zero)
df_session_base.replace([np.inf, -np.inf], 0, inplace=True)
df_session_base.fillna(0, inplace=True)

# Preview
df_session_base[[
    "user_id",
    "num_trips",
    "num_hotel_trips",
    "num_flight_trips",
    "num_bundle_trips",
    "hotel_hunter_index",
    "flight_fanatic_index",
    "bundle_index"
]].head()

"""# Session-level data was aggregated to user level, and multiple feature tables were merged into a unified user feature dataset containing engagement, trip behavior, cancellations, and preference indices."""

# 1) Base session features
df_user_base = (
    df_session_base_for_feature
    .groupby("user_id")
    .agg(
        num_sessions=("session_id","count"),
        num_empty_sessions=("empty_session","sum"),
        num_clicks=("page_clicks","sum"),
        avg_clicks_per_session=("page_clicks","mean"),
        avg_session_duration=("session_duration","mean"),
        std_session_duration=("session_duration","std")
    )
    .reset_index()
)

df_user_base["std_session_duration"] = df_user_base["std_session_duration"].fillna(0)

# 2) Merge all other user-level feature tables at once
dfs_to_merge = [df_user_base1, df_user_base2, df_user_base3]

for d in dfs_to_merge:
    df_user_base = df_user_base.merge(d, on="user_id", how="left")

# 3) Add indices
indices = (
    df_session_base
    [["user_id","hotel_hunter_index","flight_fanatic_index","bundle_index"]]
    .drop_duplicates("user_id")
)

df_user_base = df_user_base.merge(indices, on="user_id", how="left")

# save dataframe
df_user_base.to_csv(f'{directory}/user_base.csv', index=False)
df_user_base.describe().T

"""# Build Final User Base Feature Data Frame

session features <- canceled features <- trip features <- static features
"""

# a master user dataset created also I validated its structure and completeness
df_user_base_final = pd.merge(df_user_base,df_user_base1, on='user_id', how='left')
df_user_base_final = pd.merge(df_user_base_final, df_user_base2, on='user_id', how='left')
df_user_base_final = pd.merge(df_user_base_final, df_user_base3, on='user_id', how='left')

print(df_user_base_final.shape)
print(df_user_base_final.columns)
print(df_user_base_final.isna().sum())
df_user_base_final.head()

# save dataframe
df_user_base.to_csv(f'{directory}/user_base.csv', index=False)
df_user_base.describe().T
##print(df_user_base_final.shape) ## 31 columns not 32

# file downloaded
from google.colab import files

# Define the full path to the file
file_path = f'{directory}/user_base_final.csv'

# Download the file